JMLR Workshop and Conference Proceedings Gaussian Processes Practice Gaussian Process Approximations Stochastic Differential Equations edric Archambeau Department Computer Science University College London Gower Street London United Kingdom Dan Cornford Neural Computing Research Group Aston University Aston Triangle Birmingham United Kingdom Manfred Opper Artificial Intelligence Group Technical University Berlin Franklinstra Berlin Germany John Shawe Taylor Department Computer Science University College London Gower Street London United Kingdom archambeau ucl cornford aston opperm berlin jst ucl Editor Neil Lawrence Anton Schwaighofer and Joaquin Quin onero Candela Abstract Stochastic differential equations arise naturally range contexts from financial environmental modeling Current solution methods are limited their representation the posterior process the presence data this work present novel Gaussian process approximation the posterior measure over paths for general class stochastic differential equations the presence observations The method applied two simple problems the Ornstein Uhlenbeck process which the exact solution known and can compared and the double well system for which standard approaches such the ensemble Kalman smoother fail provide satisfactory result Experiments show that our variational approximation viable and that the results are very promising the variational approximate solution outperforms standard Gaussian process regression for non Gaussian Markov processes Keywords Dynamical Systems Stochastic Processes Bayesian Inference Gaussian Processes Introduction Stochastic differential equations are used wide range applications environmental modeling engineering and biological modeling They typically describe the time dynamics the evolution state vector based the approximate physics the real system together with driving noise process The noise process can thought several ways often represents processes not included the model but present the real system our work are motivated problems arising numerical weather prediction models however the methods are developing have far more general relevance Archambeau Cornford Opper and Shawe Taylor Archambeau Cornford Opper and Shawe Taylor provide brief insight into the motivation for our work consider numerical weather prediction models which are based discretization coupled set partial differential equations the dynamics which govern the time evolution the atmosphere described terms temperature pressure velocity etc Haltiner and Williams However the dynamics not include all the physical processes acting the atmosphere such radiation and clouds these are included often empirical parametrizations the model physics These dynamical models typically have state vectors with dimension and are currently treated deterministic models although there increasing recognition that full stochastic treatment necessary for progress made probabilistic weather forecasting Seiffert This work the first step move towards methods that will able treat such large complex models fully probabilistic framework particular issue address this paper the use observations together with dynamics defined stochastic differential equation infer the posterior distribution the state the system process often referred meteorology data assimilation Kalnay not review data assimilation methods extensively here but note that recently much work has been done address the issue the propagation the uncertainty initial time through the nonlinear model equations The most popular sequential method called the ensemble Kalman filter simplified Monte Carlo approach Evensen and van Leeuwen Whitaker and Hamill whereas more advanced techniques include Bayesian sequential MCMC methods Golightly and Wilkinson widely used alternative sequential treatments the data assimilation problem the called DVAR method which seeks find the most probable model trajectory over given time window typically using the model equations strong constraint using simple variational approach Courtier this work seek variational Bayesian treatment the dynamic data assimilation problem particular focus the issue defining Gaussian process approximation the temporal evolution the solution general stochastic differential equation with additive noise and the posterior approximation given the observations expect the variational nature this approximation make possible for apply these methods very large models exploiting localization hierarchical models and sparse representations Seeger present the results our initial work which focuses theoretical developments These are applied two commonly used stochastic processes the OrnsteinUhlenbeck process and the noisy double well system illustrate their application Further work required before these ideas can applied complex models such those used weather forecasting The issues and contributions raised this work are follows want the modeling properly have take into account that general the prior process nonGaussian process Therefore cannot deal with the prior efficient way just cannot deal with the posterior Thus this work significantly different from the Gaussian process methods recently extensively studied Machine Learning see for example Csato and Opper Seeger more specific cannot compute any prior moment marginal exactly When the process Markovian not necessarily time homogeneous any marginal can expressed the product the transition probabilities Even for the prior this would require the solution Fokker Planck equation which partial differential equation For almost all realistic problems the solution the corresponding Gaussian Process Approximations Stochastic Differential Equations exact Fokker Planck equation practice impossible need make approximations Risken Making approximations solve very difficult problems not new idea Machine Learning However because can always explicitly compute all prior marginals observations and test points for Gaussian process Csat and Opper essentially nearly all current work this direction boils down the approximation multivariate but finite dimensional posterior density Thus the important feature that process infinite dimensional almost never plays role inference this work things are different many areas Machine Learning are using the variational method approximating intractable probability distribution tractable one Jaakkola Beal Winn contrast most other works factorizing density does not seem make sense infinite setting are thus working with Gaussian variational densities This has mostly been ignored the Machine Learning literature The Kullback Leibler divergence Kullback and Leibler between the approximating posterior process and the exact one one between processes between probability measures over paths which makes the computation non trivial the field data assimilation such setting not new and there has been some work done for computing approximate predictions Eyink Apte These papers however not provide natural framework for estimating unknown model parameters while can attempt this variational bound for the probability observed data which can used within maximum likelihood framework start Section with review the basic setting which are working Section develop the variational approximation methods for the general class problems introduced Section show how compute the divergence between the true processes and the approximate Gaussian process and derive expressions for the posterior moments the approximating process use these expressions derive the relevant Euler Lagrange equations for the problem Section show how the variational approximation can used smoother algorithm approximate the conditional distribution the state given series observations within certain time frame and the process stochastic differential equation Section show results applying the method two example problems Ornstein Uhlenbeck process and the noisy double well system conclude Section Basic setting Consider finite set dimensional noisy observations dimensional hidden state assumed that the time evolution described Ito stochastic differential equation SDE where for simplicity assume that diag diagonal with nonlinear function and standard multivariate Wiener process not attempt any rigorous presentation such processes which would involve proper Ito calculus this paper but rather resort intuitive picture where understand such process appropriate limit discrete time process precise use the Euler Maruyama representation Archambeau Cornford Opper and Shawe Taylor the time increment and denotes sequence independent Gaussian random vectors Note that the the noise scales with which necessary obtain the non trivial limit diffusion process Such stochastic processes are widely used physics and finance model continuous random systems that evolve continuously over time Brownian motion and other diffusions fact can viewed limiting case random walk the time increment goes zero The non trivial scaling also prohibits from writing the SDE using ordinary derivatives because sample paths are continuous but not differentiable with probability one Note that this form can used for approximate the sense discretizing differential equation generation samples from the prior process Kloeden and Platen The process continuous time but unfortunately non Gaussian nonlinear Markov process not required that this process time homogeneous defines probability measure psde over paths where time interval over which would like perform our inference usual the presence observations the posterior measure given dppost dpsde where are observed the discrete times and the normalizing constant The likelihood assumed have the form multivariate Gaussian density where defines linear transformation and the noise covariance matrix future work will generalize this arbitrary nonlinear observations operators Variational approximation consider the approximation the true posterior measure Gaussian measure Gaussian process such that the divergence between the two minimized will construct such measure the following idea since the posterior process Markovian will also use Gaussian Markov process its approximation The assumption Gaussianity implies that such process must governed linear SDE where The matrix and the vector are functions optimized the variational approach They must time dependent account for the non stationarity the process caused the observations Note that the use the same noise variance Gaussian Process Approximations Stochastic Differential Equations for both processes not restriction because different choice would lead infinite divergences The divergence the two measures over the time interval computed Appendix giving with ppost Esde Eobs and Esde Eobs where the Dirac function and indicates the expectation with respect the marginal distribution the process time usual the fact that ppost gives upper bound far have not used the assumption that linear Gaussian Process posterior moments evaluate the expression the divergence and permit its subsequent minimization need the functional dependency the variational parameter functions and the marginal distributions the process any time For Gaussian fixed form variational can write One might expect that the time evolution can described set ordinary differential equations ODEs for the mean and the covariance matrix shown Appendix these are given Variational approximation the posterior order compute the parameters and the required moments minimize the divergence subject the constraints and with respect independent variations and Therefore look for the stationary points the following Lagrangian SAT Archambeau Cornford Opper and Shawe Taylor where and are Lagrange multipliers Note that matrix symmetric allow for explicit variation perform integration parts which gives SAT the latest time only take variations with respect and such that and For simplicity also fix the values and rather than optimizing them Hence taking the derivatives with respect and leads respectively the following Euler Lagrange equations follows from and that the variational functions and each time can expressed function the Lagrange multipliers and well the moments and where have used the fact that xxT mmT and for any Gaussian along with the following results xxT Smoothing algorithm propose the following smoothing algorithm Make some initial guesses for and and choose sufficiently small relaxation parameter Repeat until the reaches its minimum value Solve and forward time for fixed variational parameters and well fixed and Gaussian Process Approximations Stochastic Differential Equations With and found for solve backward time with and Esde Esde When there observation use the following jump conditions HTR HTR The amplitude the jumps are found evaluating the derivatives Eobs with respect and time Meanwhile update the variational parameters follows where The underlying motivation for using the updates and rather than directly using and avoid numerical instabilities due possibly too large update steps then the complexity the smoothing algorithm where the number sweeps iterations and Experiments this section the approach validated two dimensional examples the OrnsteinUhlenbeck process and the double well system The process mathematical model the velocity particle undergoing Brownian motion Uhlenbeck and Ornstein here considered reference example Actually know the exact solution for the kernel covariance function which induced the corresponding prior Gaussian Markov process One can thus perform standard Gaussian Process regression Rasmussen and Williams for computing the posterior process The variational approximation leads this case the same exact result contrast the system standard data assimilation benchmark see Miller Eyinck and Restrepo Its prior Markov process non Gaussian there are two equally likely equilibria resulting psde being bimodal Therefore the posterior process necessarily non Gaussian but can well approximated Gaussian one given appropriate observations Ornstein Uhlenbeck process The SDE the Ornstein Uhlenbeck process defined follows xdt Archambeau Cornford Opper and Shawe Taylor where the drift parameter The induced stationary covariance kernel given This kernel can plugged into the regression formulae compute the exact posterior process and make prediction unseen data Next let consider the approximation the scalar SDE defined the linear function The variational fixed points parameters are given where and denote the scalar Lagrange multipliers The parameters are updated after each set forward and backward passes The forward pass consists propagating the mean and the variance using the discretized Euler Lagrange equations corresponding and The backward pass uses the discretized ODEs the Lagrange multipliers along with the jump conditions The specific ODEs are given dEsde dEsde where Esde Figure shows realization the process and compares the posterior solution found regression and the variational method The true states are corrupted zeromean Gaussian noise The noise levels are assumed known Observe how the resulting smoothers are identical except the first observation where have set and for initializing the smoothing algorithm Figure shows the time evolution the variational parameters and the Lagrange multipliers after convergence Due the jump conditions the value the Lagrange multipliers jumps when there are observations does the value the variational parameters Nevertheless the posterior process smooth and continuous over time but not differentiable times where there are observations Double well system The second example that consider the double well system which highly nonlinear The force arises from double well potential The SDE given Gaussian Process Approximations Stochastic Differential Equations regression Variational approximation Figure Ornstein Uhlenbeck example The true process indicated grey and the noisy observations are marked crosses The left panel shows the expected posterior process solid and the standard deviation tube dashed obtained regression while the right one shows the same quantities obtained the Gaussian variational approximation Figure Variational parameters and Lagrange multipliers time Archambeau Cornford Opper and Shawe Taylor where Due the driving noise the solution fluctuates around one the two minima located Occasionally however larger fluctuations arise possibly leading the transition the other well Therefore the associated Markov process non Gaussian For the fixed point variational parameters the approximate SDE have The ODEs describing the time evolution the Lagrange multipliers are then dEsde dEsde where Esde The jump conditions are given and the experiments consider the same sample the one considered Miller Eyink and the parametrization identical the time step equal there are noisy observations the time window the variance the observation noise equal and the driving noise equal Also only one transition occurs the time window First compare the variational solution the one obtained regression using the kernel and the standard RBF kernel see Figure The observation noise set its true value The drift parameter and the RBF kernel width are selected the ones maximizing the evidence that the marginal probability the observations Both regressors are able locate the transition However they are not able estimate the wells accurately Note also how the using the kernel overestimates the posterior covariance These results are not surprising the GPs not make use the knowledge the dynamics and assume the observation noise small contrast the variational solution expected much closer the true posterior process Indeed good solution needs not necessarily closer the true history but its most probable value The noise tube also more informative this case Qualitatively this solution the same the one obtained Eyink who recently introduced alternative mean field approximation approach data assimilation These authors also noted that the popular ensemble Kalman smoother fails correctly locate the transition Eyinck matter fact lags one measurement and this failure not cured the backward pass Figure shows the evolution the divergence function the number sweeps for different values the under relaxation parameter which has effect the convergence rate The higher the faster the algorithm converges However when too large numerical instabilities may occur leading the smoothing algorithm diverge For reasonable values the algorithm converges relatively fast that less than sweeps Finally note that the evolution the Lagrangian identical the evolution the function the number sweeps This can understood noticing that the constraints and are satisfied construction after each forward sweep Gaussian Process Approximations Stochastic Differential Equations with the kernel Variational approximation with the RBF kernel Sample path Figure Double well system and show the regression solution for two different kernels while the optimal variational solution Shows the true history sample path The solid lines are the posterior means and the dashed ones the posterior means the posterior standard deviation Archambeau Cornford Opper and Shawe Taylor Number iterations Figure Double well system Evolution the divergence function the number iterations sweeps for different values the under relaxation parameter solid dash dash dot dot Conclusion this paper have introduced novel variational approximation the posterior distribution system governed general stochastic differential equation The main innovation the work that the posterior distribution over paths rather than finite dimensional multivariate posterior standard Gaussian process inference have shown how incorporate observations into the approximation scheme proposing smoothing algorithm Results applying the method two example systems are very promising the one hand the approach consistent the variational solution identical the exact solution when the stochastic process Gaussian one the other hand the method able cope with strongly nonlinear systems for example inducing multimodal probability measures contrast most approximate state the art techniques This work represents initial step towards the application variational Bayesian inference methods general stochastic differential equation based models Much remains done enable the apply these methods the very large complicated models used weather forecasting see for example Dance for discussion current issues Future work will explore further the links between our methods and the cutting edge data assimilation methods developped Eyink Apte and will focus better smoothing algorithm One area expect make progress the application our methods spatially distributed systems since are able control the complexity the posterior approximation defining the class and representations linear models used our method for example constraining have some simplified Gaussian Process Approximations Stochastic Differential Equations form such sparse representation tri diagonal form Another interesting application our methods would situations where the system model known only approximately but the magnitude the model errors not well known employing our variational framework would allow make inference the model error represented which would produce better estimates the posterior uncertainty after data assimilation Acknowledgments would like thank the organizers the Gaussian Processes Practice meeting for putting together such interesting workshop and providing with the opportunity present this work would also like thank Hauke Tschach for checking the equations the experimental part This work has been funded the EPSRC part the Variational Inference for Stochastic Dynamic Environmental Models VISDEM project Appendix Kullback Leibler divergence along state path derive the result for the divergence use the following discrete time heuristics derivation continuous time using Girsanovs theorem Kloeden and Platen will given elsewhere Consider the discretized version the SDE and the corresponding version for the approximate linear one where drawn from multivariate Gaussian density with identity covariance Hence the probability density discrete time path sequence generated the true prior process without observations and the approximate process posterior are respectively given The divergence between and given psde dxk dxk dxk Archambeau Cornford Opper and Shawe Taylor where have used the fact that possible pass the continuum limit the time interval because all terms have the ordinary linear scaling with Riemann sums can shown that had not assumed that both processes have the same noise variance the corresponding sum would have diverged Hence the limit obtain the divergence between the two probability measures for state paths this time interval psde where indicates the expectation with respect which the marginal density time Appendix Ordinary differential equations for the parameters The ODEs the means and the covariance matrices follow from when neglecting terms beyond first order where have used the fact that Wiener process such that and dWT dtI References Amit Apte Martin Hairer Andrew Stuart and Jochen Voss Sampling the posterior approach non Gaussian data assimilation Physica Submitted available from http www maths warwick stuart sample html Matthew Beal Variational Algorithms for Approximate Bayesian Inference PhD thesis Gatsby Computational Neuroscience Unit University College London Courtier Thepaut and Hollingsworth strategy for operational implementation VAR using incremental approach Quarterly Journal the Royal Meteorological Society Lehel Csat and Manfred Opper Sparse line Gaussian processes Neural Computation Sarah Dance Issues high resolution limited area data assimilation for quantitative precipitation forecasting Physica Geir Evensen and Peter van Leeuwen ensemble Kalman smoother for nonlinear dynamics Monthly Weather Review Gaussian Process Approximations Stochastic Differential Equations Gregory Eyinck and Juan Restrepo Most probable histories for nonlinear dynamics tracking climate transitions Journal Statistical Physics Gregory Eyinck Juan Restrepo and Francis Alexander statistical mechanical approach data assimilation for nonlinear dynamics Evolution approximations Journal Statistical Physics Accepted Gregory Eyink Juan Restrepo and Francis Alexander mean field approximation data assimilation for nonlinear dynamics Physica Andrew Golightly and Darren Wilkinson Bayesian sequential inference for nonlinear multivariate diffusions Statistics and Computing appear George Haltiner and Roger Williams Numerical Prediction and Dynamic Meteorology John Wiley and Sons Chichester Tommi Jaakkola Tutorial variational approximation methods Manfred Opper and David Saad editors Advanced Mean Field Methods Theory and Practice The MIT Press Eugenia Kalnay Atmospheric modeling data assimilation and predictability Cambridge University Press Cambridge Peter Kloeden and Eckhard Platen Numerical Solution Stochastic Differential Equations Springer Verlag Berlin Kullback and Leibler information and sufficiency Annals Mathematical Statistics Robert Miller Michael Ghil and Francois Gauthiez Advanced data assimilation strongly nonlinear dynamical systems Journal the Atmospheric Sciences Carl Rasmussen and Christopher Williams Gaussian Processes for Machine Learning The MIT Press Cambridge Massachusetts Risken The Fokker Planck equation methods solutions and applications SpringerVerlag Berlin Matthias Seeger Gaussian processes for Machine Learning International Journal Neural Systems Matthias Seeger Neil Lawrence and Ralf Herbrich Efficient nonparametric Bayesian modelling with sparse Gaussian process approximations Submitted for Journal Publication Rita Seiffert Richard Blender and Klaus Fraedrich Subscale forcing global atmospheric circulation model and stochastic parameterisation Quarterly Journal the Royal Meteorological Society accepted Archambeau Cornford Opper and Shawe Taylor Uhlenbeck and Ornstein the theory Brownian motion Physical Review Jeffrey Whitaker and Thomas Hamill Ensemble data assimilation without perturbed observations Monthly Weather Review John Winn Variational Message Passing and its Applications PhD thesis Department Physics University Cambridge 