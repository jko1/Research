{
 "metadata": {
  "name": "",
  "signature": "sha256:28ea32d806cf7930a31dab359b5109ba3382517f5dba2b0d3add47ea4e1f108f"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import BeautifulSoup\n",
      "import urllib2\n",
      "import os\n",
      "import re\n",
      "from BeautifulSoup import BeautifulSoup as Soup\n",
      "\n",
      "base_page = 'http://machinelearning.wustl.edu/mlpapers'\n",
      "html = urllib2.urlopen(base_page + '/years').read()\n",
      "home_page = Soup(html)\n",
      "data_icml = home_page.findAll('body')[0].findAll('a')[4::]\n",
      "for item in data_icml:\n",
      "    year = item.renderContents().split(\"(\")[0]\n",
      "    year_link = base_page + item.get('href')[1::]\n",
      "    base_path = \"/Users/jessicako/statnews/wustl\"\n",
      "    new_dir = base_path + \"/\" + year + \"_pdf\"\n",
      "    print year\n",
      "    # if used for getting certain years.\n",
      "    if int(year) >= 2010:\n",
      "        if not os.path.exists(new_dir):\n",
      "            os.makedirs(new_dir)\n",
      "        html = urllib2.urlopen(year_link).read()\n",
      "        year_page = Soup(html)\n",
      "        papers = year_page.findAll('b')\n",
      "        for paper in papers:\n",
      "            paper = paper.findAll('a')[0]\n",
      "            paper_link = base_page + paper.get('href')[2::]\n",
      "            print paper_link\n",
      "            html = urllib2.urlopen(paper_link).read()\n",
      "            pdf_html = Soup(html)\n",
      "            pdf_link = pdf_html.findAll('b')[0].findAll('a')[0]\n",
      "            pdf_link_end = pdf_link.get('href')  # end of pdf link\n",
      "            name = pdf_link.renderContents()\n",
      "            name = name.lower()\n",
      "            name = re.sub(r'[^a-zA-Z0-9\\[\\]\\(\\)\\s]', '', name).replace(' ', '-')\n",
      "            name = pdf_link_end.split('/')[-1][:-4:] + \".\" + name + \".pdf\"  # name of file to make\n",
      "            pdf_end = pdf_link_end[3::]\n",
      "            if \"doi.acm\" in pdf_end:\n",
      "                acm_page = \"http://\" + pdf_end[4::]\n",
      "                hdr = {'User-Agent': 'Mozilla/5.0'}\n",
      "                req = urllib2.Request(acm_page, headers=hdr)\n",
      "                html = urllib2.urlopen(req).read()\n",
      "                pdf_html = Soup(html)\n",
      "\n",
      "                if len(pdf_html.findAll(title='FullText PDF')) == 0:\n",
      "                    print \"did not find \" + name\n",
      "                    continue\n",
      "                temp_link = pdf_html.findAll(title='FullText PDF')[0].get('href')\n",
      "                actual_pdf_link = \"http://dl.acm.org\" + \"/\" + temp_link\n",
      "            else:\n",
      "                actual_pdf_link = base_page + \"/\" + pdf_link_end.strip(\".\")\n",
      "       # where the pdf is at\n",
      "            if not os.path.exists(new_dir + \"/\" + name):\n",
      "                print actual_pdf_link\n",
      "                hdr = {'User-Agent': 'Mozilla/5.0'}\n",
      "                req = urllib2.Request(actual_pdf_link, headers=hdr)\n",
      "                u = urllib2.urlopen(req)\n",
      "                localFile = open(new_dir + \"/\" + name, 'w')\n",
      "                localFile.write(u.read())\n",
      "                localFile.close()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}